---
layout: default
title: Deep Learning - 01 - Machine Learning Review
---

# Chapter 1: Machine Learning Review

So what exactly is deep learning, anyway? The phrase is fairly vague and means different things to different people depending on who you talk to. First, though, it's good to have a basic understanding of typical machine learning tasks and pipelines to understand how deep learning is different.

## Machine Learning Tasks

Machine learning broadly is the task of modelling data, usually with some kind of numerical or statistical model. The first key distinction between machine learning tasks is between **supervised** and **unsupervised** learning:

- **Supervised learning** is function approximation. 
    - Input:
        - data $$X$$
        - labels $$Y$$
        - paired examples $$(x,y)$$
    - Assume:
        - there exists a function that maps from data to labels $$f: X \to Y$$
        - our paired examples $$(x,y)$$ satisfy $$f(x) = y$$
    - Learn: approximation $$h$$ such that $$h(x) \approx f(x)$$
- **Unsupervised learning** is modelling the distribution of data
    - Input:
        - data $$X$$
    - Learn:
        - **clusters**: groupings of related data points
        - a transformation to a different feature space that preserves relationships between data points
        - a generating function or probability distribution $$g$$ such that statistically $$X$$ appears to be drawn from $$g$$: $$X \sim g$$

### Supervised Learning

Supervised learning encompasses algorithms for function approximation. Given a dataset $$X$$ and a function $$f$$ that takes elements of the dataset and produces output $$y = f(x)$$, learn a function $$h$$ such that $$h(x) \approx f(x)$$.

{% include image
    src="figs/mnist_digits.png"
    alt="Examples of MNIST digits. A 10x16 grid of handwritten digits, each row is a different digit 0-9."
    attribution="By Josef Steppan - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=64810040"
    caption="Examples digits from the MNIST dataset, 28x28 pixel images of handwritten digits 0-9. MNIST is a common benchmark for computer vision tasks although it is a fairly easy to attain high accuracies."
%}

This "labelling" function $$f$$ can be obvious, like trying to predict the price of a car from attributes of the car like the make, model, year, mileage, condition, etc. In this case the true function $$f$$ is the process a car salesman goes through to put a price on a car given those attributes. We are trying to create an approximate function $$h$$ that takes the same attributes and assigns a similar price.

For some tasks it can be more opaque, like predicting today's weather from yesterday's weather. In the case of the weather, there is an underlying physical process but it is not a clear function that only takes as input the past day's weather. In reality, the weather is determined by a function (the unfolding of the laws of physics) acting on a set of data (the physical conditions of our planet).

In the case of weather prediction, the physical conditions of the planet are what's known as a **latent variable** or hidden variable. They are not fully observed or recorded but do affect the outcome. Our model can try to account for these variables or simply circumvent them. In either case we are trying to build an approximation of a function that doesn't actually exist, we simply assume it does. There's a lot of things like that in machine learning. Don't let it bother you too much!

#### Classification vs Regression

There is often a distinction drawn between **classification** and **regression** tasks in supervised learning:

- **Classification**:
    - labels are discrete classes
    - \\(Y \in \mathbb{Z}\\)
    - example algorithm: logistic regression
- **Regression**:
    - lables are real-valued
    - \\(Y \in \mathbb{R}\\)
    - example algorithm: linear regression

It's pretty confusing that both of those example algorithms have "regression" in their name, huh? There are more complicated or mixed tasks as well that involve predicting both discrete and continuous variables. We'll worry about those later.


### Unsupervised Learning

In unsupervised learning there are no "labels", there is only data. Typically unsupervised learning assumes the data is drawn from some statistical distribution or generating function. Tasks in unsupervised learning involve estimating the distribution or function the data is drawn from, learning to represent the data in a different way (usually by transforming the attribute space), or finding groupings of related data points based on the similarity of their attributes.

#### K-means Clustering 

**K-means** clustering is a common technique for clustering data based on similarity of attributes. Data points are clustered into groups based on their distance from each other in attribute space according to some metric. The algorithm for K-means is an example of iterative **expectation-maximization (EM)** algorithm for finding a local maximum of a latent-variable model.

The k-means algorithm assumes:

- there are some number \(k\) of clusters in the data
- each data point belongs to a cluster
    - cluster assignments are latent variables, they are not observed
- each cluster has a central point, or **centroid**
- data points in a cluster are closer to that cluster's centroid than to any other cluster's centroid

First k-means randomly initialzes cluster centroids. Then the algorithm alternates between **expectation** and **maximization** steps:

**Expectation**: assign data to clusters based on distance to nearest centroid. Each data point is assigned to the cluster of the nearest centroid by some given distance metric (often, but not always, \(L_2\) distance.

**Maximization**: centroids are updated based on the data that belongs to their cluster. Typically this is done by assigning the centroid to be the mean of the data points assigned to that cluster (hence the "means" in k-means).

K-means clustering can be a useful tool for analyzing data sets, discovering patterns, and leveraging those patterns to accomplish some task. For example, k-means clustering on the color values of pixels in an image separates pixels into clusters based on visual similarity, giving a segmentation mask that groups visually similar elements. These elements may correspond to objects or continuous structures.

{% include image
    src="figs/kmeans.png"
    alt="Example of k-means clustering on image pixels by color. K = 2. On the left, an image of a rose. On the right, a two color image where the pink of the rose has been segmented as a separate group from the green of the background."
    attribution=""
    caption="K-means clustering on pixels by \(L_2\) distance with \(K=2\)."
%}


<!--
Weather Prediction Notation:
- $$X$$: yesterday's weather
- $$Y$$: today's weather
- $$f$$: hypothetical function mapping $$X \to Y$$
- $$Y$$: physical conditions of planet/universe yesterday
- $$T$$: physical conditions of planet/universe today
- $$p$$: laws of physics, $$p: Y \to T$$
- $$w$$: interpretation of physical conditions as weather, $$w: T \to Y$$
-->

## Feature Extraction

Machine learning relies on data. A data point is a collection of **attributes**. These attributes can be:

- binary 
    - is the car new?
    - is it a convertible?
- discrete
    - what brand is the car?
    - what color?
- continuous
    - how many miles per gallon does the car get?
    - how much does it weigh?
- or even more complicated...
    - a paragraph describing the car in natural english
    - a video of the car driving on a bumpy road

Machine learning algorithms usually want data in a particular format. For example, decision trees partition the data into discrete categories to make predictions thus can handle discrete attributes on their data. However, logistic regression multiplies the data attributes by a weight matrix to make predictions thus the input data should be continuous.

If we want to perform logistic regression on a data set that has discrete attributes we need to encode them somehow. One possibility is **one-hot encoding**. One-hot encoding converts a single, discrete attribute with $$n$$ different possibilities into a binary vector with $$n$$ elements.

If the cars in our dataset can be "black", "green", "blue", or "red", a one-hot encoding of this attribute would assign a black car the vector $$[1,0,0,0]$$, a green car the vector $$[0,1,0,0]$$, etc.

One-hot encoding is an example of **feature extraction**. Feature extraction is the process of taking raw data and converting it into useable and useful attributes for a machine learning model. Most machine learning algorithms rely heavily on feature extraction. K-means clustering needs data attributes to be in a metric space where we can compute distances. Logistic regression needs continuous data. **Bayes networks** assume that data attributes are conditionally independent from each other. Each of these restrictions can be addressed by extracting the right features in the right way from raw data.

## Deep Learning is Trainable Feature Extraction

Hand-designed feature extraction can be very powerful but also very tedious. **Deep learning** encompasses a set of algorithms that process (relatively) raw data instead of curated features. These algorithms learn to extract features from the raw data and then use those features to make predictions.

Typically deep learning is:

- Neural network based
- Uses large amounts of data
- Incorporates feature extraction as part of the model
    - Has many "layers" of processing
    - Early layers extract simple features from raw data
    - Later layers extract complex features from simple features

This is _very_ exciting for machine learning practitioners. Typically the difference between a good and bad machine learning model comes down to the features the model uses. Good features = good model, or, as they say, "garbage in, garbage out". Deep learning offers a different path, instead of trying to find what features make a good model, let the model learn and decide for itself.

So far deep learning has been most successful with data that has some inherent **structure** and the algorithms take advantage of that structure. Images are composed of pixels and nearby pixels are statistically more related to each other than far away pixels. Natural language is a string of words where future words depend on past words. Sound is a waveform composed of oscillations at different frequencies with those frequencies changing over time. These are the domains where deep learning (currently) works well.

In domains with less structure (for example, diagnosing an illness based on a patient's symptoms) there are many algorithms that outperform neural networks or deep learning. For those tasks you are much better off using gradient-boosted decision trees or random forests.

## Learning From Data



